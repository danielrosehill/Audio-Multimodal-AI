# Data Set and Repository for Audio-Supported Multimodal Models

> The purpose of this dataset and repository is to create an initial analysis data gathering of multimodal models with audio support, currently classified under 'multimodal' on Hugging Face. The intention is to sift through available options and understand the advantages of audio multimodal models over classic STT models.

*Transcribed: 7 Dec 2025 17:17*

---

The purpose of this data set and repository is to create a initial analysis data gathering of a small, relatively small sub classification of multimodal models, which are multimodal models with audio supports. Uh, currently, in the 7th of December 2025, date of creation, they're classified in Hugging Face under the multimodal category, which is interesting, because they're not under the audio category. The audio category has ASR, automatic speech recognition, which includes both the uh, classic speech to text models, like Whisper, Paki, swave to vec and others, as well as all the accessories, like pinose, diarization, turn detection, voice activity detection, all the large and small components that together um create very powerful AI enhanced transcription workflows. The difference between these classic, even though they're very new, classic STT models and audio multimodal is very significant in my view at least. The audio multimodal models um have the are distinguished by the ability to have a innate understanding of audio. To process audio tokens and understand them in conjunction with a text prompt. An example of a non open source uh implementation is Gemini, which since Gemini 2 through to Gemini 3, so Gemini 2, Gemini 2.5 and Gemini 3, have all had multimodal support for the audio modality. Each modality within multimodal opens up interesting new use cases. For example, um the mainstream, or the most mainstream multimodal modality being image, or vision models where you can understand, or the model can understand data uh provided by an image. A more, much more advanced and still more, much more emerging modality is is uh video, requiring the model to process frames in real time. The audio multi, the audio aspect of of the multimodal picture has kind of gone under the radar in my opinion. And I think that that's a great pity, which is why I uh created this collection, because both from my own usage usage um and implementations, I wanted to sift through what was available. And um of course as always for anyone else who wants to uh use the information, they are more than welcome to uh for whatever um purpose they wish. The advantage, the very fundamental difference between speech to text, for example, Whisper, and audio multimodal is the ability to provide a prompt which will really has a uh fast amount of potential uses. It could be here is an audio file. Can you detect and describe what accent the, um thanks. User has. Can you describe the user's voice parametrically or just using natural language? But in the transcription use case, which is the home of speech to text, uh transcribing speech into into words, um it can be used to do speech to text, because it's processing audio tokens that may contain speech. Um, and also guide the text that emerges from that process. So rather than in traditional STT, we the baseline functionality, before any other processing is just translate the speech into text or and then different things like punctuation being added on top of that, but that's the foundational principle. With audio multimodal, it's a unified inference uh process and endpoint that um enables for a transcription. An example might be transcribe what I'm recording, and sending up to Gemini, which is me speaking off the cuff, and providing um my notes for transcription. So if I run this through Whisper, I'll just get back what I said. I can do Whisper plus um open AI for to say transcribe then reformat this as a as a console as a summarized version of this in a read me format. But if I send this to an audio multimodal model like Gemini 3, um, I can do all of that at one go. What really impressed me lately, and this is verging into longer, longer notes than I intended about this, but just to um maybe share why I think this particular sub classification is so interesting. Um, I always wondered whether there was some limit in terms of how long this could be used for, because um audio really spans the gamut. If it's you know, 3 minutes might be a voice note. 60 minutes might be uh a YouTube video. It could be 3 hours for that matter. Uh conference proceedings, I don't know. Uh so what I did, I had a particularly long voice note about 1 hour and it managed to transcribe that, which means that firstly you can fit that that input within the uh the input buffer, or the uh upload size constraint. And Gemini 3 with the advantage of long, um long output, long maximum output uh limits. That can, that actually to me is quite a almost a a game changing use case, because it means that instead of, you know, I have a choice. Do I want to go to Whisper, let's say for 1 hour conference. I can go to Whisper and it's on YouTube and for some reason I wanted to say I wasn't at this conference, but uh Whisper, can you summarize this? So then it's going to do the the traditional chain. But it's very elegant, um that with Gemini, I can write a single API call that can say that can provide the system prompt, you're you're a specialist in summarizing, you know, long conference panels, you get the audio, you do the transformation and you return that summarized text. And that's where multimodal uh is hugely useful. I think it's a brilliant use case for stuff like um voice journals. You might be keeping a a food journal of your allergies, and that's following a predictable format and you don't want to just transcribe the format. So it goes beyond the text cleanup uh like take out the ms, add the paragraphs, add the punctuation, which are things that multimodal can all do better than in my opinion experience so far, the combination. And it's also just kind of um frustrating, or not frustrating, but it's complicated to have to add speech to text, VAD, punctuation restoration, all these little sort of mini elements just to get what you're looking for. So, um I think even from that sort of simplifying the engineering standpoint, it's uh it's really there's I I I predict, I'll put my uh stake my claim that I think these are this is the future of uh of voice AI, or at least this modality for transcribing speech. Uh I think this this this will be the successor to the first wave of speech to text models. Um, so regarding the actual what I'm what I'm going to be doing, I'll just because everything in AI is so quick that you can't, you really need to always I I always record the dates because by two weeks time, it's this is already old information. So I'm going to um fill up just a few folders or fill up a folder with a export from the Hugging Face API and using Claude I was actually able to recurse from the category list into the individual models to pull out the parameter information, uh which for those who want to try this out on local hardware is pretty much essential information I think. And that's raw data in CSV and JSON, and just looking at what's here and going the the whole purpose of this exercise from there's always when I ever whenever I open source things there's usually a me motivation and a share motivation. The me the me motivation is I want to try uh get a transcription um workflow uh running through one of these and that requires looking through what's available to figure out uh what the best option it is. Um, and the sharing is just anyone who wants to do the same process. But what I'm going to be doing in my sort of looking through these or my analysis is it's actually very, I think it's actually a good thing that there's only 151 models. And that's actually a staggering ratio. There is about 2500 ASR models. So basically, what's that? 151. Um, that's a huge ratio. So they're much, much smaller niche. And I'm sure ASR, there's not really 25,000 unique ASR models, but there is it's a there's many, many more of those. Um, and likewise in Hugging Face, um I think a good thing about it and also kind of a bad thing is that you get this very long list and it takes a bit of sort of analysis to actually see what's really going on, because it's it's it's you know, there's not even 151 models, there is a much smaller number than that and then there's just a lot of quantizations and variance. So um I'm gathering together basically this list to look through it and just to firstly understand uh what are the actual models, the unique ones we're seeing. So off just off the cuff here from first glance. Quen 2 Audio 7B. Um there is an instruct version of that and there is GGUF. Uh step audio Lama 3.2, interesting, with audio. But I'm more interested actually in the in the long tail always of uh the kind of more soundwave, like I've never never heard of this model company or or their model before. Um, so that'll be kind of what I'm looking for is uh is seeing uh what the actual what the field is like at this uh at this uh point in time and then seeing if I can actually try to use any of these, which would be superb uh if I could. So that's the that's those are the notes for the repository and um I'll periodically update this as I go along.

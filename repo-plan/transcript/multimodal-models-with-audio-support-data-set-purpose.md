# Multimodal Models with Audio Support: Data Set Purpose

> This data set aims to analyze multimodal models, specifically those with audio support, which are classified in hugging face under the multimodal category. It explores the differences between classic speech-to-text models and audio multimodal models, focusing on the latter's ability to process audio tokens and understand them in conjunction with text prompts.

*Transcribed: 7 Dec 2025 17:17*

---

The purpose of this data set and repository is to create an initial analysis data gathering of a small, relatively small subclassifcation of multimodal models which are multimodal models with audio support. Currently, the 7th of December 2025, date of creation, they are classified in hugging face under the multimodal category, which is interesting because they're not under the audio category.

## Audio Categories

The audio category has ASR, automatic speech recognition, which includes both the classic speech-to-text models like Whisper, Parakeet, Swav2Vec and others, as well as all the accessories like pi-note-ization, turn detection, voice activity detection, all the large and small components that together create very powerful AI enhanced transcription workflows.

## Classic STT Models vs. Audio Multimodal

The difference between these classic—even though they're very new—classic STT models and audio multimodal is very significant in my view, at least. The audio multimodal models have the—are distinguished by the ability to have an innate understanding of audio, to process audio tokens and understand them in conjunction with a text prompt. An example of a non-open source implementation is Gemini, which since Gemini 2 through to Gemini 3, so Gemini 2, Gemini 2.5 and Gemini 3, have all had multimodal support for the audio modality.

Each modality within multimodal opens up interesting new use cases. For example, the mainstream or the most mainstream multimodal modality being image or vision models where you can understand or the model can understand data provided by an image. A much more advanced and still more—much more emerging modality—is video, requiring the model to process frames in real-time.

The audio—multi—the audio aspect of the multimodal picture has kind of gone under the radar in my opinion, and I think that's a great pity, which is why I created this collection because both from my own usage and implementations, I wanted to sift through what was available. And, of course, it's always for anyone else who wants to use the information, they are more than welcome to for whatever purpose they wish.

## Key Advantages of Audio Multimodal

The advantage, the very fundamental difference between speech to text, for example, Whisper, and audio multimodal is the ability to provide a prompt which will really has a vast amount of potential uses. It could be, "Here's an audio file, can you detect and describe what accent the user has? Can you describe the user's voice parametrically or just using natural language?"

But in the transcription use case, which is the home of speech to text, transcribing speech into into words, it can be used to do speech to text because it's processing audio tokens that may contain speech and also guide the text that emerges from their process. So rather than in traditional STT, we—the baseline functionality before any other processing is just translate the speech into text or—and then different things like punctuation being added on top of that, but that's the foundational principle.

With audio multimodal, it's a unified inference process and endpoint that enables—for a transcription, an example might be transcribe what I'm recording and sending up to Gemini, which is me speaking off the cuff and providing my notes for transcription. So if I run this through Whisper, I'll just get back what I said. I can do Whisper plus OpenAI 4 to say transcribe, then reformat this as a as a consult as a summarized version of this in a readme format. But if I send this to an audio multimodal model like Gemini 3, I can do all of that at one go.

What really impressed me lately, and this is verging into longer—longer notes than I intended about this, but just to—maybe share why I think this particular subclassifcation is so interesting. I always wondered whether there were some limits in terms of how long this could be used for because audio really spans the gamut. If it's, you know, three minutes might be a voice note, 60 minutes might be a YouTube video. It could be three hours for that matter. Conference proceedings, I don't know. So what I did, I had a particularly long voice note about one hour and it managed to transcribe that, which means that firstly you can fit that input within the—the input buffer or the upload size constraint. And Gemini 3 with the advantage of long—long output—long maximum output limits, that can—that actually to me is quite almost a—a game changing use case because it means that instead, you know, I have a choice, do I want to go to Whisper? Let's say for a one hour conference. I can go to Whisper and it's on YouTube, and for some reason I wanted to say I wasn't at this conference, but Whisper, can you summarize this? So then it's going to do the the traditional chain. But it's very elegant that with Gemini I can write a single API call that can say—that can provide the system prompt, "You're—you're a specialist in summarizing, you know, long conference panels. You get the audio, you do the transformation, and you return that summarized text." And that's where multimodal is hugely useful. I think it's a brilliant use case for stuff like voice journals. You might be keeping a a food journal of your allergies, and that's following a predictable format and you don't want to just transcribe the format. So it goes beyond the text cleanup, like, "Take out the ums, add the paragraphs, add the punctuation," which are things that multimodal can all do better than in my opinion, experience so far, the combination.

And it's also just kind of frustrating—or not frustrating, but it's complicated to have to add speech to text, VAD, punctuation restoration, all these little sort of mini elements just to get what you're looking for. So I think even from that sort of simplifying the engineering standpoint, it's—it's really—there's—I—I—I predict, I'll put my—stake my claim that I think these are—this is the future of voice AI or at least this modality for transcribing speech. I think this—this—this will be the successor to the first wave of speech to text models.

## Repository Plans

So regarding the actual what I'm—what I'm going to be doing, I'll just—because everything in AI is so quick that you can't—you really need to always—I—I always record the dates because by two weeks' time, it's—this is already old information. So I'm going to fill up just a few folders or fill up a folder with a export from the hugging face API and using Claude, I was actually able to recurse from the category list into the individual models to pull out the parameter information, which for those who want to try this out on local hardware is pretty much essential information, I think. And that's raw data in CSV and JSON and just looking at what's here and going—the—the whole purpose of this exercise from—there's always when I ever—whenever I open source things, there's usually a me motivation and a share motivation. The me—the me motivation is I want to try get a transcription workflow running through one of these, and that requires looking through what's available to figure out what the best option is. And the sharing is just anyone who wants to do the same process. But what I'm going to be doing in my sort of looking through these or my analysis is it's actually very—I think it's actually a good thing that there's only 151 models. And that's actually a staggering ratio. There is about 2,500 ASR models. So basically, what's that? 151? That's a huge ratio. So they're a much—much smaller niche. And I'm sure ASR, there's not really 25,000 unique ASR models, but there is—it's there's many—many more of those. And likewise, in hugging face, I think a good thing about it and also kind of a bad thing is that you get this very long list, and it takes a bit of sort of analysis to actually see what's really going on because it's—it's—it's, you know, there's not even 151 models. There is a much smaller number than that, and then there's just a lot of quantizations and variants. So I'm gathering together basically this list to look through it and just to firstly understand what are the actual models, the unique ones we're seeing. So off just off the cuff here from first glance, Quen2 Audio 7B, there is an instruct version of that and there is GGUF.

Step Audio Lama 3.2, interesting, with audio. But I'm more interested actually in the—in the long tail always of—the kind of more Soundwave, like I've never—never heard of this model company or—or their model before. So that'll be kind of what I'm looking for is is seeing what the actual—what the field is like at this—at this point in time, and then seeing if I can actually try to use any of these, which would be superb if I could. So that's the—that's—those are the notes for the repository. And I'll periodically update this as I go along.
